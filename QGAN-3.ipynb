{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hKaxQUEALFg"
   },
   "source": [
    " PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricare il dataset di anomaly detection\n",
    "#dataset = pd.read_csv('../../KDDTrain+.txt')\n",
    "dataset = pd.read_csv('../DATA/nsl-kdd/KDDTrain+.txt')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = (['duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment','urgent','hot'\n",
    ",'num_failed_logins','logged_in','num_compromised','root_shell','su_attempted','num_root','num_file_creations'\n",
    ",'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login','count','srv_count','serror_rate'\n",
    ",'srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate','diff_srv_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count'\n",
    ",'dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate','dst_host_srv_diff_host_rate','dst_host_serror_rate'\n",
    ",'dst_host_srv_serror_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate','outcome','level'])\n",
    "dataset.columns = columns\n",
    "dataset.loc[dataset['outcome'] == \"normal\", \"outcome\"] = 'normal'\n",
    "dataset.loc[dataset['outcome'] != 'normal', \"outcome\"] = 'attack'\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Colonne categoriche da escludere dal RobustScaler\n",
    "cat_cols = ['is_host_login','protocol_type','service','flag','land',\n",
    "            'logged_in','is_guest_login', 'level', 'outcome']\n",
    "\n",
    "# Preprocessing numerico + encoding categoriale\n",
    "def preprocess(df_train, df_test):\n",
    "    # Separazione numeriche\n",
    "    num_train = df_train.drop(cat_cols, axis=1)\n",
    "    num_test = df_test.drop(cat_cols, axis=1)\n",
    "    num_cols = num_train.columns\n",
    "\n",
    "    # Scaling robusto solo sul training set\n",
    "    scaler = RobustScaler()\n",
    "    scaled_train = scaler.fit_transform(num_train)\n",
    "    scaled_test = scaler.transform(num_test)\n",
    "\n",
    "    df_train_scaled = pd.DataFrame(scaled_train, columns=num_cols, index=df_train.index)\n",
    "    df_test_scaled = pd.DataFrame(scaled_test, columns=num_cols, index=df_test.index)\n",
    "\n",
    "    # Ricostruzione dataframe\n",
    "    df_train_final = df_train.drop(num_cols, axis=1).copy()\n",
    "    df_test_final = df_test.drop(num_cols, axis=1).copy()\n",
    "\n",
    "    df_train_final[num_cols] = df_train_scaled\n",
    "    df_test_final[num_cols] = df_test_scaled\n",
    "\n",
    "    # One-hot encoding delle categoriche\n",
    "    df_train_final = pd.get_dummies(df_train_final, columns=['protocol_type', 'service', 'flag'])\n",
    "    df_test_final = pd.get_dummies(df_test_final, columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "    # Allineamento colonne dopo one-hot (test potrebbe non avere tutte le categorie)\n",
    "    df_test_final = df_test_final.reindex(columns=df_train_final.columns, fill_value=0)\n",
    "\n",
    "    return df_train_final, df_test_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split prima del preprocessing\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42, stratify=dataset['outcome'])\n",
    "\n",
    "# Preprocessing numerico e categorico\n",
    "train_processed, test_processed = preprocess(train_df.copy(), test_df.copy())\n",
    "\n",
    "# Split X/y\n",
    "X_train = train_processed.drop(columns=['outcome']).values\n",
    "y_train = train_processed['outcome'].values\n",
    "\n",
    "X_test = test_processed.drop(columns=['outcome']).values\n",
    "y_test = test_processed['outcome'].values\n",
    "\n",
    "# Scaling MinMax (post-preprocessing, pre-PCA)\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_train_scaled = minmax_scaler.fit_transform(X_train)\n",
    "X_test_scaled = minmax_scaler.transform(X_test)\n",
    "\n",
    "# PCA solo su training\n",
    "n=8\n",
    "n_components = n\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# (opzionale) Riscalatura anche dopo PCA\n",
    "X_train_pca = minmax_scaler.fit_transform(X_train_pca)\n",
    "X_test_pca = minmax_scaler.transform(X_test_pca)\n",
    "\n",
    "# Variance ratio (quanto \"spiega\" ogni componente)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Se vuoi convertire in dataframe:\n",
    "#train_pca_df = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "#train_pca_df['Outcome'] = y_train\n",
    "\n",
    "#test_pca_df = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "#test_pca_df['Outcome'] = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola il 5% dei dati disponibili per ciascuna classe nel training set\n",
    "n_train = int(len(X_train_pca[y_train == 'normal']) * 0.02)\n",
    "#n_attack_train = int(len(X_train_pca[y_train == 'attack']) * 0.05)\n",
    "\n",
    "# Calcola il 5% dei dati disponibili per la classe 'normal' nel test set\n",
    "n_test = int(len(X_test_pca[y_test == 'normal']) * 0.02)\n",
    "\n",
    "# Seleziona i dati per il training\n",
    "benign_data_T = X_train_pca[y_train == 'normal'][:n_train]\n",
    "\n",
    "# Seleziona i dati per il test\n",
    "benign_data_Ttest = X_test_pca[y_test == 'normal'][:n_test]\n",
    "outlier_dataT = X_test_pca[y_test == 'attack'][:n_test]\n",
    "\n",
    "# Stampa le forme dei dataset selezionati\n",
    "print(\"Benigni per training:\", benign_data_T.shape)\n",
    "print(\"Attacchi per test:\", outlier_dataT.shape)\n",
    "print(\"Benigni per test:\", benign_data_Ttest.shape)\n",
    "\n",
    "# Visualizza il primo esempio dei benigni per training\n",
    "print(\"Primo benigno per training:\", benign_data_T[0])\n",
    "print(\"Primo benigno per test:\", benign_data_Ttest[0])\n",
    "print(\"Primo maligno per test:\", outlier_dataT[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7jF8lDrtW74"
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pennylane as qml\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wa9zp3GX7jzF",
    "outputId": "927d4f18-e6ae-4f57-a3a0-b5b87a22317f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# creazione di un DataLoader con dati normali\n",
    "dataloader = DataLoader(benign_data_T, batch_size=128, shuffle=True)\n",
    "all_batches = []\n",
    "# iterare sui batch\n",
    "for batch_features in dataloader:\n",
    "    print(\"Batch features shape:\", batch_features.shape)  # Es. torch.Size([32, 4])\n",
    "    all_batches.append(batch_features)\n",
    "\n",
    "all_real_data = torch.cat(all_batches, dim=0)\n",
    "print(all_real_data.shape)\n",
    "#print(benign_data_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creazione di un DataLoader con dati di test reali\n",
    "dataloaderTestReal = DataLoader(benign_data_Ttest, batch_size=128, shuffle=True)\n",
    "all_batches = []\n",
    "# iterare sui batch\n",
    "for batch_features in dataloaderTestReal:\n",
    "    print(\"Batch features shape:\", batch_features.shape)  # Es. torch.Size([32, 4])\n",
    "    all_batches.append(batch_features)\n",
    "\n",
    "all_real_dataTest = torch.cat(all_batches, dim=0)\n",
    "print(all_real_dataTest.shape)\n",
    "\n",
    "# creazione di un DataLoader con dati di test di attacco\n",
    "dataloaderTestOut = DataLoader(outlier_dataT, batch_size=128, shuffle=True)\n",
    "all_batches = []\n",
    "# iterare sui batch\n",
    "for batch_features in dataloaderTestOut:\n",
    "    print(\"Batch features shape:\", batch_features.shape)  # Es. torch.Size([32, 4])\n",
    "    all_batches.append(batch_features)\n",
    "\n",
    "all_out_dataTest = torch.cat(all_batches, dim=0)\n",
    "print(all_out_dataTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lchJS8KtsfiJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminatore completamente connesso per dati numerici\"\"\"\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): Numero di caratteristiche (feature) in input.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Strato di input -> primo hidden layer\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            # Primo hidden layer -> secondo hidden layer\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            # Secondo hidden layer -> output (probabilità)\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90mEIsz05sVK"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parametri del generatore\n",
    "n_qubits = n  # Numero di qubit\n",
    "q_depth = 4  # Profondità del circuito quantistico\n",
    "\n",
    "# Quantum simulator\n",
    "dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "# Enable CUDA device if available\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "#feature map\n",
    "def basic_feature_map(x):\n",
    "    for i in range(n_qubits):\n",
    "        qml.Hadamard(wires=i)\n",
    "    for i in range(n_qubits):\n",
    "        qml.RZ(x[i], wires=i)\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "        qml.RZ((x[i] * x[i + 1]), wires=i + 1)\n",
    "        \n",
    "def basic_feature_map1(x):\n",
    "    for i in range(n_qubits):\n",
    "        qml.Hadamard(wires=i)\n",
    "    for i in range(n_qubits):\n",
    "        qml.RZ(x[i], wires=i)\n",
    "        qml.RY(x[i], wires=i)\n",
    "    # Rimuovo le rotazioni non lineari eccessive\n",
    "    # oppure aggiungo entanglement leggero:\n",
    "    \n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "\n",
    "def basic_feature_map2(x):\n",
    "    for i in range(n_qubits):\n",
    "        qml.Hadamard(wires=i)\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(x[i], wires=i)\n",
    "    # Entanglement ciclico leggero\n",
    "    for i in range(n_qubits):\n",
    "        qml.CZ(wires=[i, (i+1) % n_qubits])\n",
    "\n",
    "\n",
    "        \n",
    "# === ANSATZ EfficientSU2 ===\n",
    "def efficient_su2(weights, wires):\n",
    "    num_qubits = len(wires)\n",
    "    depth = weights.shape[0]\n",
    "\n",
    "    for layer in range(depth - 1):\n",
    "        for i in range(num_qubits):\n",
    "            qml.RY(weights[layer, i, 0], wires=wires[i])\n",
    "            qml.RZ(weights[layer, i, 1], wires=wires[i])\n",
    "\n",
    "        # Entanglement lineare tra i qubit\n",
    "        for i in range(num_qubits - 1):\n",
    "            qml.CNOT(wires=[wires[i], wires[i + 1]])\n",
    "        qml.CNOT(wires=[wires[-1], wires[0]])\n",
    "\n",
    "    # Blocco finale di rotazioni\n",
    "    for i in range(num_qubits):\n",
    "        qml.RY(weights[-1, i, 0], wires=wires[i])\n",
    "        qml.RZ(weights[-1, i, 1], wires=wires[i])\n",
    "\n",
    "\n",
    "def two_local_ansatz(weights, wires):\n",
    "    num_qubits = len(wires)\n",
    "    depth = weights.shape[0]\n",
    "\n",
    "    for layer in range(depth):\n",
    "        # Rotazioni su ogni qubit\n",
    "        for i in range(num_qubits):\n",
    "            qml.RY(weights[layer, i, 0], wires=wires[i])\n",
    "            qml.RZ(weights[layer, i, 1], wires=wires[i])\n",
    "\n",
    "        # Entanglement lineare (a catena)\n",
    "        for i in range(num_qubits - 1):\n",
    "            qml.CZ(wires=[wires[i], wires[i + 1]])\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
    "def quantum_circuit(latent_vector, weights):\n",
    "    # Codifica del rumore latente (opzionale: feature map)\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(latent_vector[i], wires=i)\n",
    "\n",
    "    # Ansatz stile TwoLocal\n",
    "    two_local_ansatz(weights, wires=range(n_qubits))\n",
    "\n",
    "    return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knAcGfHi6QnY"
   },
   "outputs": [],
   "source": [
    "def partial_measure(noise, weights):\n",
    "\n",
    "    probs = quantum_circuit(noise, weights)  # Lista di 2^n valori di probabilità\n",
    "    num_states = 2 ** n_qubits\n",
    "\n",
    "    transformed_data = probs[:n_qubits].clone()\n",
    "    for i in range(n_qubits):\n",
    "        # Somma i valori alternati dalla riga corrente (step di i+1)\n",
    "        sum_value = 0\n",
    "        # Ciclo per sommare in blocchi alternati di i+1\n",
    "        for start in range(0, num_states,  2**(i + 1)):  # Ogni passo è di 2*(i+1)\n",
    "            sum_value += sum(probs[start:start + 2**(i)])  # Sommiamo i primi i+1 element  \n",
    "        transformed_data[i] = sum_value\n",
    "    return transformed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWZloLf89fTd"
   },
   "outputs": [],
   "source": [
    "class QuantumGenerator(nn.Module):\n",
    "    \"\"\"Generatore quantistico per dati numerici.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent_dim (int): Dimensione del vettore latente.\n",
    "        \"\"\"\n",
    "        super(QuantumGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Inizializza i parametri del circuito quantistico\n",
    "        self.q_params = nn.Parameter(0.1 * ((torch.rand(q_depth, n_qubits,2) - 0.5) * torch.pi), requires_grad=True)\n",
    "        \n",
    "\n",
    "    def forward(self, batch_size):\n",
    "\n",
    "        epsilon = 0.5\n",
    "        generated_data = []\n",
    "        for _ in range(batch_size):\n",
    "            # Campiona un vettore latente casuale\n",
    "            #latent_vector = torch.full((self.latent_dim,), torch.pi ) * (torch.rand(self.latent_dim) - 0.5)   #torch.rand(self.latent_dim) * (torch.pi)\n",
    "            #latent_vector = torch.rand(self.latent_dim)\n",
    "            latent_vector = torch.rand(latent_dim, device=device) * (math.pi)\n",
    "            \n",
    "            #latent_vector = (torch.rand(self.latent_dim) - 0.5) * np.pi / 2  # range [-π/4, π/4]\n",
    "            \n",
    "            # Ottieni l'output del circuito quantistico\n",
    "            probs = partial_measure(latent_vector, self.q_params)\n",
    "\n",
    "             # Convert probs to float32\n",
    "            probs = probs.type(torch.float32)\n",
    "            generated_data.append(probs)\n",
    "\n",
    "\n",
    "        generated_data = torch.stack(generated_data, dim=0)\n",
    "        return generated_data.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS0Yb3yERAv8",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "# Funzione per confrontare le distribuzioni\n",
    "def compare_distributions(real_data, fake_data, epoch):\n",
    "    real_data = real_data.cpu().numpy()\n",
    "    # Se fake_data è un tensore PyTorch, convertilo in NumPy\n",
    "    if isinstance(fake_data, torch.Tensor):\n",
    "        fake_data = fake_data.cpu().numpy()\n",
    "    print(real_data.shape)\n",
    "    print(fake_data.shape)\n",
    "    print(fake_data)\n",
    "    num_features = real_data.shape[1]\n",
    "\n",
    "    # Crea un subplot per ogni feature\n",
    "    fig, axes = plt.subplots(1, num_features, figsize=(5 * num_features, 4))\n",
    "    if num_features == 1:\n",
    "        axes = [axes]  # Assicura che funzioni anche con una singola feature\n",
    "\n",
    "    for i in range(num_features):\n",
    "        sns.kdeplot(real_data[:, i], label=\"Real\", ax=axes[i], color=\"blue\")\n",
    "        sns.kdeplot(fake_data[:, i], label=\"Generated\", ax=axes[i], color=\"orange\")\n",
    "        axes[i].set_title(f\"Feature {i+1}\")\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.suptitle(f\"Epoch {epoch} - Distribution Comparison\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_probabilities(probs1):\n",
    "    \"\"\"\n",
    "    Genera un grafico a barre per visualizzare le 16 probabilità di uscita del circuito quantistico.\n",
    "\n",
    "    Args:\n",
    "        probs\n",
    "    \"\"\"\n",
    "    # Ottieni le probabilità dal circuito quantistico\n",
    "    probs=probs1.detach().numpy()\n",
    "\n",
    "    # Indici degli stati (da 0000 a 1111 in binario)\n",
    "    states = [format(i, '04b') for i in range(len(probs))]\n",
    "\n",
    "    # Creazione del grafico\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(states, probs, color='royalblue', alpha=0.7)\n",
    "\n",
    "    # Etichette e titolo\n",
    "    plt.xlabel(\"Stati Quantistici (10 Qubit)\")\n",
    "    plt.ylabel(\"Probabilità\")\n",
    "    plt.title(\"Distribuzione delle Probabilità del Circuito Quantistico\")\n",
    "    plt.ylim(0, 1)  # Imposta il limite massimo a 1\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Mostra i valori sopra le barre\n",
    "    for i, p in enumerate(probs):\n",
    "        plt.text(i, p + 0.02, f\"{p:.2f}\", ha='center', fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_losses(generator_losses, discriminator_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(generator_losses, label=\"Generator Loss\", color='blue')\n",
    "    plt.plot(discriminator_losses, label=\"Discriminator Loss\", color='red')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_discriminator_metrics(discriminator, dataloadreal, dataloaderout, device):\n",
    "    y_true, y_pred = [], []\n",
    "    y_true2, y_pred2 = [], []\n",
    "    with torch.no_grad():\n",
    "        for real_data in dataloadreal:\n",
    "            real_data = real_data.to(device).float()\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            real_labels = torch.ones(batch_size, device=device)\n",
    "            #fake_data = generator(batch_size).to(device)\n",
    "            #fake_labels = torch.zeros(batch_size, device=device)\n",
    "\n",
    "            res= discriminator(real_data).view(-1)\n",
    "            real_preds = res > 0.5\n",
    "\n",
    "            #print(f\"real_preds; {res}\")\n",
    "\n",
    "            y_true.extend(real_labels.cpu().numpy())\n",
    "            y_pred.extend(real_preds.cpu().numpy())\n",
    "\n",
    "        for out_data in dataloaderout:\n",
    "            out_data = out_data.to(device).float()\n",
    "            batch_size = out_data.size(0)\n",
    "\n",
    "            fake_labels = torch.zeros(batch_size, device=device)\n",
    "\n",
    "            res= discriminator(out_data).view(-1)\n",
    "            fake_preds = res > 0.5\n",
    "\n",
    "            #print(f\"fake_preds; {res}\")\n",
    "\n",
    "            y_true2.extend(fake_labels.cpu().numpy())\n",
    "            y_pred2.extend(fake_preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    accuracy2 = accuracy_score(y_true2, y_pred2)\n",
    "    precision2 = precision_score(y_true2, y_pred2)\n",
    "    recall2 = recall_score(y_true2, y_pred2)\n",
    "    f12 = f1_score(y_true2, y_pred2)\n",
    "\n",
    "\n",
    "    return accuracy, precision, recall, f1, accuracy2, precision2, recall2, f12\n",
    "\n",
    "def compute_generator_metrics(discriminator, generator, batch_size, device):\n",
    "    with torch.no_grad():\n",
    "        fake_data = generator(batch_size).to(device)\n",
    "        fake_preds = discriminator(fake_data).view(-1) > 0.5\n",
    "\n",
    "        y_true = torch.ones(batch_size, device=device).cpu().numpy()\n",
    "        y_pred = fake_preds.cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def calculate_fid(real_features, generated_features):\n",
    "    # Calcola la media e la covarianza per entrambi i dataset\n",
    "    mu_real, sigma_real = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu_gen, sigma_gen = np.mean(generated_features, axis=0), np.cov(generated_features, rowvar=False)\n",
    "\n",
    "    # Calcola la distanza di Frechet\n",
    "    fid = np.sum((mu_real - mu_gen) ** 2) + np.trace(sigma_real + sigma_gen - 2 * sqrtm(sigma_real @ sigma_gen))\n",
    "    \n",
    "    # A volte sqrtm restituisce valori complessi, quindi prendiamo solo la parte reale\n",
    "    if np.iscomplexobj(fid):\n",
    "        fid = fid.real\n",
    "\n",
    "    return fid\n",
    "\n",
    "def plot_fid(epochs_recorded, fid_scores):\n",
    "        # Disegna il grafico FID\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epochs_recorded, fid_scores, marker='o', linestyle='-', color='b', label=\"FID Score\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"FID Score\")\n",
    "        plt.title(\"FID Score over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_2fid(epochs_recorded, fid_scores, genFid):\n",
    "  \n",
    "    # Disegna il grafico FID per entrambi i valori\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    # Plotta il FID originale\n",
    "    plt.plot(epochs_recorded, fid_scores, marker='o', linestyle='-', color='b', label=\"FID Score\")\n",
    "\n",
    "    # Plotta il secondo FID (genFid)\n",
    "    plt.plot(epochs_recorded, genFid, marker='s', linestyle='--', color='r', label=\"Generated FID\")\n",
    "\n",
    "    # Etichette e titolo\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"FID Score\")\n",
    "    plt.title(\"FID Score over Epochs\")\n",
    "\n",
    "    # Aggiunge la legenda\n",
    "    plt.legend()\n",
    "\n",
    "    # Mostra la griglia\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Mostra il grafico\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy(epochs, d_accuracies, g_accuracies,d_accuracy2):\n",
    "    \"\"\"\n",
    "    Plotta l'accuracy del Discriminatore e del Generatore nel tempo.\n",
    "\n",
    "    Parametri:\n",
    "    - epochs: lista delle epoche\n",
    "    - d_accuracies: lista dei valori di accuracy del discriminatore\n",
    "    - g_accuracies: lista dei valori di accuracy del generatore\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, d_accuracies, label=\"Discriminatore Reali\", marker='o', linestyle='-')\n",
    "    plt.plot(epochs, g_accuracies, label=\"Generatore\", marker='s', linestyle='--')\n",
    "    plt.plot(epochs, d_accuracies2, label=\"Discriminatore Attacchi\", marker='o', linestyle='-')\n",
    "\n",
    "    plt.xlabel(\"Epoche\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Andamento dell'Accuracy nel tempo\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJhtpqva9yYm",
    "outputId": "c8612235-be23-413b-d277-4c1414c03ed8"
   },
   "outputs": [],
   "source": [
    "# Inizializza il generatore\n",
    "latent_dim = n_qubits  # Dimensione del vettore latente\n",
    "generator = QuantumGenerator(latent_dim=latent_dim)\n",
    "\n",
    "# Genera un batch di dati\n",
    "batch_size = len(all_real_data)\n",
    "\n",
    "# generate the data\n",
    "generated_data = generator(batch_size=batch_size)\n",
    "\n",
    "# Convert the generated_data to float32 before using it.\n",
    "generated_data = generated_data.type(torch.float32)\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "print(\"Dati generati:\")\n",
    "#print(generated_data)\n",
    "print(generated_data.shape)\n",
    "with torch.no_grad():\n",
    "    compare_distributions(all_real_data, generated_data , 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_data[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jgqvHMZDRJPT",
    "outputId": "afa84c4f-bf5d-4ea6-8829-c9b72ffafe9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Iperparametri più stabili\n",
    "lrd = 0.0005  # learning rate\n",
    "lrg = 0.005  # learning rate\n",
    "b1 = 0.7  # first momentum parameter\n",
    "b2 = 0.999  # second momentum parameter\n",
    "\n",
    "\n",
    "num_iter = 500\n",
    "#batch_size = 128 ... modificare in dataloader\n",
    "\n",
    "\n",
    "# Numero di feature\n",
    "input_size = n\n",
    "\n",
    "# Inizializzazioni\n",
    "discriminator = Discriminator(input_size=input_size).to(device)\n",
    "generator = QuantumGenerator(latent_dim=n_qubits).to(device)\n",
    "\n",
    "#modalita addestramento\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ottimizzatori più stabili beta1 0.5 meno inerzia, beta2 0.999 velocita aggiornamento pesi\n",
    "#optD = optim.Adam(discriminator.parameters(), lr=lrD, betas=(0.5, 0.999))\n",
    "#optG = optim.Adam(generator.parameters(), lr=lrG, betas=(0.5, 0.999))\n",
    "optG = optim.Adam(generator.parameters(), lr=lrg, betas=(b1, b2), weight_decay=0.005) # to avoid overfitting\n",
    "optD = optim.Adam(discriminator.parameters(), lr=lrd, betas=(b1, b2), weight_decay=0.005)\n",
    "\n",
    "#optD = optim.SGD(discriminator.parameters(), lr=lrD)\n",
    "#optG = optim.SGD(generator.parameters(), lr=lrG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "#raccolta tutti i dati reali\n",
    "#all_real_data = torch.cat([data[0] for data in dataloader], dim=0).to(device).float()\n",
    "\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "fid_scores = []\n",
    "epochs_recorded = []\n",
    "genfid_scores = []\n",
    "d_accuracies = []  # Lista delle accuracy del discriminatore\n",
    "d_accuracies2 = []\n",
    "g_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_iter):\n",
    "    total_d_loss = 0\n",
    "    total_g_loss = 0\n",
    "\n",
    "    for i, real_data in enumerate(dataloader):\n",
    "        real_data = real_data.to(device)\n",
    "        current_batch_size = real_data.size(0)\n",
    "        #128 modifica in dataloader\n",
    "\n",
    "        # da double a float32\n",
    "        real_data = real_data.float()\n",
    "        \n",
    "        # Azzera i gradienti\n",
    "        optD.zero_grad()\n",
    "\n",
    "        # Dati reali\n",
    "        real_output = discriminator(real_data).view(-1)\n",
    "        real_labels = torch.ones(current_batch_size, 1, device=device, dtype=torch.float32).view(-1)\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "\n",
    "        # Genera dati falsi\n",
    "        #noise = torch.randn(current_batch_size, 4, device=device) //noise generato in funzione quantumgenerator\n",
    "        fake_data = generator(current_batch_size).detach()\n",
    "\n",
    "        # Dati falsi\n",
    "        fake_labels = torch.zeros(current_batch_size, 1, device=device, dtype=torch.float32).view(-1)\n",
    "        fake_output = discriminator(fake_data).view(-1)\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Perdita discriminatore\n",
    "        real_loss.backward()\n",
    "        fake_loss.backward()\n",
    "        d_loss = real_loss + fake_loss\n",
    "        optD.step()\n",
    "\n",
    "        # Addestramento generatore //senza .detach\n",
    "        optG.zero_grad()\n",
    "        fake_data = generator(current_batch_size)\n",
    "        generator_output = discriminator(fake_data).view(-1)\n",
    "\n",
    "        # Il generatore cerca di ingannare il discriminatore crietrion/obbeittivo: dati generati classsficiati come real 1\n",
    "        g_labels = torch.ones(current_batch_size, 1, device=device, dtype=torch.float32).view(-1)\n",
    "        g_loss = criterion(generator_output, g_labels)\n",
    "\n",
    "        #print(\"Gradients before loss.backward():\", generator.q_params.grad)\n",
    "        g_loss.backward()\n",
    "\n",
    "        #print(\"Gradients after loss.backward():\", generator.q_params.grad)\n",
    "\n",
    "\n",
    "        optG.step()\n",
    "        #print(\"Gradients after optimizer step:\", generator.q_params.grad)\n",
    "\n",
    "\n",
    "        # Accumula le perdite\n",
    "        total_d_loss += d_loss.item()\n",
    "        total_g_loss += g_loss.item()\n",
    "\n",
    "    discriminator_losses.append(total_d_loss / (i + 1))\n",
    "    generator_losses.append(total_g_loss / (i + 1))\n",
    "\n",
    "\n",
    "    # Stampa medie delle perdite\n",
    "    if 1:\n",
    "        print(f\"Epoch {epoch}/{num_iter}, \"\n",
    "              f\"Avg Loss D: {total_d_loss/(i+1):.4f}, \"\n",
    "              f\"Avg Loss G: {total_g_loss/(i+1):.4f}\")\n",
    "            # Plot delle distribuzioni ad ogni epoca\n",
    "        \n",
    "    if epoch % 1 == 0: #mostra plot ogni 50epoch\n",
    "      with torch.no_grad():\n",
    "        fake_data_sample = generator(len(all_real_data))  # Genera lo stesso numero di allrealdata\n",
    "        compare_distributions(all_real_data, fake_data_sample, epoch)\n",
    "        fid_score = calculate_fid(all_real_data.cpu().numpy(), fake_data_sample.cpu().numpy())\n",
    "        print(f\"FID Score: {fid_score}\")\n",
    "        \n",
    "\n",
    "        random_data = np.random.rand(len(all_real_data), n_qubits)\n",
    "        genFid = calculate_fid(all_real_data.cpu().numpy(), random_data)\n",
    "        print(f\"genFID score; {genFid}\")\n",
    "        #compare_distributions(all_real_data, random_data, epoch)\n",
    "\n",
    "        # Salva il valore di FID e l'epoca corrispondente\n",
    "        fid_scores.append(fid_score)\n",
    "        genfid_scores.append(genFid)\n",
    "        epochs_recorded.append(epoch)\n",
    "        plot_2fid(epochs_recorded,fid_scores,genfid_scores)\n",
    "        plot_fid(epochs_recorded,fid_scores)\n",
    "          \n",
    "        #ks_stat, p_value = ks_2samp(all_real_data, fake_data_sample)\n",
    "        #print(f\"KS Statistic: {ks_stat}\")\n",
    "        #print(f\"P-value: {p_value}\")\n",
    "        plot_losses(generator_losses, discriminator_losses)\n",
    "\n",
    "        d_accuracy, d_precision, d_recall, d_f1, d_accuracy2, d_precision2, d_recall2, d_f12 = compute_discriminator_metrics(discriminator, dataloaderTestReal, dataloaderTestOut, device)\n",
    "        g_accuracy, g_precision, g_recall, g_f1 = compute_generator_metrics(discriminator, generator, len(dataloader.dataset), device)\n",
    "\n",
    "        print(f\"Metrics at epoch {epoch}:\")\n",
    "        print(f\"  Discriminator real- Accuracy={d_accuracy:.4f}, Precision={d_precision:.4f}, Recall={d_recall:.4f}, F1-score={d_f1:.4f}\")\n",
    "        print(f\"  Discriminator fake- Accuracy={d_accuracy2:.4f}, Precision={d_precision2:.4f}, Recall={d_recall2:.4f}, F1-score={d_f12:.4f}\")\n",
    "        print(f\"  Generator - Accuracy={g_accuracy:.4f}, Precision={g_precision:.4f}, Recall={g_recall:.4f}, F1-score={g_f1:.4f}\")\n",
    "        d_accuracies.append(d_accuracy)\n",
    "        d_accuracies2.append(d_accuracy2)  \n",
    "        g_accuracies.append(g_accuracy)\n",
    "        plot_accuracy(epochs_recorded, d_accuracies, g_accuracies,d_accuracy2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa argsort per ordinare gli indici in base ai valori crescenti\n",
    "min_indices = np.argsort(fid_scores)[:50]\n",
    "mean_gen = np.mean(genfid_scores)\n",
    "\n",
    "print(f\"media fid generato {mean_gen}\")\n",
    "# Stampa posizione e valore corrispondente\n",
    "print(\"Primi 10 minimi (posizione -> valore):\")\n",
    "for idx in min_indices:\n",
    "    print(f\"Posizione: {idx}, Valore: {fid_scores[idx]}\")\n",
    "\n",
    "\n",
    "# Definisci la dimensione dell'intervallo\n",
    "bin_size = 50\n",
    "\n",
    "# Trova il valore massimo per sapere quanti intervalli servono\n",
    "max_index = max(min_indices)\n",
    "num_bins = (max_index // bin_size) + 1  # numero di intervalli necessari\n",
    "\n",
    "\n",
    "# Inizializza contenitori\n",
    "counts = [0] * num_bins\n",
    "values_per_bin = [[] for _ in range(num_bins)]\n",
    "\n",
    "# Raccoglie valori per ogni intervallo\n",
    "for idx in min_indices:\n",
    "    bin_index = idx // bin_size\n",
    "    counts[bin_index] += 1\n",
    "    values_per_bin[bin_index].append(fid_scores[idx])\n",
    "\n",
    "# Stampa risultati con media\n",
    "for i in range(num_bins):\n",
    "    start = i * bin_size\n",
    "    end = start + bin_size - 1\n",
    "    count = counts[i]\n",
    "    valori = values_per_bin[i]\n",
    "    media = np.mean(valori) if valori else 0\n",
    "    print(f\"Intervallo {start}-{end}: {count} indice/i, media valori = {media:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capire se riducendo i dati prima di fare pca cambia il risultato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capire come fa a partire da un accuracy cosi alta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#migliorare il generatore e cambiare i LR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
